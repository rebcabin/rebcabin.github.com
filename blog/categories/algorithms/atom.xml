<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Algorithms | The Light Cone]]></title>
  <link href="http://rebcabin.github.com/blog/categories/algorithms/atom.xml" rel="self"/>
  <link href="http://rebcabin.github.com/"/>
  <updated>2013-02-05T09:01:02-08:00</updated>
  <id>http://rebcabin.github.com/</id>
  <author>
    <name><![CDATA[Brian Beckman (rebcabin = brianbec)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Welford's Way to Kalman City]]></title>
    <link href="http://rebcabin.github.com/blog/2013/02/04/welfords-better-formula/"/>
    <updated>2013-02-04T06:48:00-08:00</updated>
    <id>http://rebcabin.github.com/blog/2013/02/04/welfords-better-formula</id>
    <content type="html"><![CDATA[<p>Should you square differences or should you difference squares?
Both methods harbor algebraically correct ways to compute <a href="http://localhost:4000/blog/2013/01/22/covariance-matrices/">incremental (co)variance</a>, 
but their numerical properties are different, as pointed out by
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/">John D. Cook here</a> and <a href="http://www.johndcook.com/blog/2008/09/28/theoretical-explanation-for-numerical-results/">here</a>.</p>

<p>When you square numbers, you do two risky things: you make big numbers
bigger and you make small numbers smaller.  Together, these effects
can be <a href="http://en.wikipedia.org/wiki/Loss_of_significance">catastrophic</a>.
Suppose, for instance, that you’re working with positive numbers that
look like $b+\epsilon$, where $b\geq 1$ and $0&lt;\epsilon&lt;1$. Squaring,
you get $b^2+2b\epsilon+\epsilon^2$. The big part, $b$, gets bigger:
$b^2$; and the small part, $\epsilon$, gets smaller:
$\epsilon^2$. Everyone knows you shouldn’t add big numbers to little
ones on computers.  The big ones crowd out space for the little ones.
You will eventually run out of <em>dynamic range</em> – the number of bits
in your machine numbers – and your squares will evaluate to
$b^2+2b\epsilon$ or even just to $b^2$.  This is the dreaded
<em>underflow</em>.</p>

<p>Remember those Mandelbrot-set-viewing programs?  If you kept zooming,
the picture would eventually get all soft and pillowy and you would
never get to see more detail.  That’s because you ran out of dynamic
range.  There wasn’t any room for all the heavenly beauty hiding in the $\epsilon^2$.
It takes special techniques to keep going; see <a href="http://www.hpdz.net/TechInfo.htm">high-precision deep zoom</a> and <a href="http://fractaljourney.blogspot.com/">fractaljourney</a>.</p>

<p>Now, when you try to subtract squares from one another, you get
another opportunity to mess up.  Everyone knows you shouldn’t subtract
nearly equal numbers on computers <em>and</em> expect the little bits to
contain good information (<a href="http://www.codeproject.com/Articles/25294/Avoiding-Overflow-Underflow-and-Loss-of-Precision">another John D. Cook article, here, elaborates</a>).
It’s another aspect of crowding out: the little bits you’re trying to
recover by subtracting were never there in the first place.</p>

<p>But why would it make a difference to square first, then subtract,
versus subtract first, then square?  If you square first and then
subtract, you’re subtracting extra-big numbers – bigger than you
need.  If you subtract first, you’re still exposed to crowding out,
but less so.  Squared numbers crowd out approximately twice the bits
of non-squared numbers.  You can avoid going to special techniques
longer if you subtract first and then square.</p>

<p>This is rather a deep and important observation, especially for sequential
estimation techniques like the <a href="http://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a>
and its many
variations. These techniques rely on accurate, iterative propagation
of covariance matrices. Losing even a little significance each round
can make a final result useless or worse, <em>i.e.</em>, not obviously wrong.</p>

<p>As a result, a whole technology of avoiding squares has arisen in the
filtering business.
<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=61004&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel4%2F9%2F2222%2F00061004.pdf%3Farnumber%3D61004">Bierman’s work</a>
is in my own lineage due to my time at the Jet Propulsion Laboratory,
and searching for any of “Square-Root Information Filter,”
“Square-Root Sigma-Point Information Filter,” “Square-Root Unscented
Kalman Filter,” and so on will uncover a robust, contemporary literature on
continuing research and applications of sophisticated methods for
avoiding the squaring of machine numbers.</p>

<p>I can see a direct provenance of these techniques in Welford’s method,
the best method cited by John above.  Welford’s is a recurrence formula,
giving the sum of $N+1$ squared residuals in terms of the sum of $N$
squared residuals, where the residuals are differences of data from
their means.  Since the means can be calculated incrementally (never
requiring storage of all the data), Welford’s is also incremental,
requiring only the storage of prior results.  Welford’s formula is
also pretty to look at and easy to remember.</p>

<p>Let $S_N$ be the sum of squared residuals of the first $N$ data:</p>

<script type="math/tex; mode=display">
S_N := \sum_{i=1}^N{\left(x_i-\bar{x}_N\right)^2}
</script>

<!--
$$
\begin{align*}
r_{i,N} &:= (x_i-\bar{x}_N) \\ 
S_N     &:= \sum_{i=1}^N{r_{i,N}^2}
\end{align*}
$$
-->

<p>where $\bar{x}_N$ is the mean of the $N$ data $x_i,\,i\in [1..N]$:</p>

<script type="math/tex; mode=display">
\bar{x}_N:=\frac{1}{N}\sum_{i=1}^{N}{x_i} := \Sigma_N \left/ N \right.
</script>

<p>Then Welford’s update formula is</p>

<script type="math/tex; mode=display">
S_{N+1} = S_N + \left(x_{N+1}-\bar{x}_N\right)\left(x_{N+1}-\bar{x}_{N+1}\right)
</script>

<!--
Just multiply the new residual 
$x_{N+1}-\bar{x}_{N+1}$ 
times the
difference of the new datum and the old mean 
$x_{N+1}-\bar{x}_N$ 
and add it to the old sum of squared residuals $S_N$.
-->

<p>Generally, I prefer to remember the derivations of formulas along with
the formulas.  Doing so gives deeper, permanent understanding plus a
chance to reuse parts of the derivation in other formulas.  If I don’t
remember the derivation, it’s a little like knowing a tune without
knowing its chord progression and verse structure.  You might still be
able to play the tune, but not with as much conviction or creativity.</p>

<p>A brute-force expansion of the terms leads to a lot of messy
algebra that all cancels out, proving the formula correct, but not
yielding any insight into how the author may have seen the pretty and
memorable form.  <a href="http://planetmath.org/OnePassAlgorithmToComputeSampleVariance.html">An article at Planetmath.org</a> 
gave me the key to the puzzle: write $S_{N+1}$ as a sum of the same terms as in $S_N$, but with a correction $\gamma$:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}

S_{N+1}
  &=\sum_{i=1}^{N+1}{\left(x_i - \bar{x}_{N+1}\right)^2} \\

  &=\sum_{i=1}^{N}{\left(x_i - \bar{x}_{N+1}\right)^2} + \left(x_{N+1}-\bar{x}_{N+1}\right)^2\\

  &=\sum_{i=1}^N{\left(x_i - \left(\bar{x}_{N} + \gamma\right)\right)^2} + \left(x_{N+1}-\bar{x}_{N+1}\right)^2

\end{align*}
 %]]&gt;</script>

<p>where</p>

<script type="math/tex; mode=display">
\gamma:=\bar{x}_{N+1}-\bar{x}_N
</script>

<p>Regroup the inner sum, then square:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}

S_{N+1}
  &=\sum_{i=1}^N{\left(\left(x_i - \bar{x}_N\right) - \gamma\right)^2} + \left(x_{N+1}-\bar{x}_{N+1}\right)^2 \\

  &=\sum_{i=1}^N{\left(\left(x_i - \bar{x}_N\right)^2 - 2\gamma\left(x_i - \bar{x}_N\right)+\gamma^2\right)} + \left(x_{N+1}-\bar{x}_{N+1}\right)^2 \\

  &=\sum_{i=1}^N{\left(x_i - \bar{x}_N\right)^2} - 2\gamma\sum_{i=1}^N{\left(x_i - \bar{x}_N\right)} + \sum_{i=1}^N{\gamma^2} + \left(x_{N+1}-\bar{x}_{N+1}\right)^2 \\

  &=S_N+N\gamma^2+\left(x_{N+1}-\bar{x}_{N+1}\right)^2

\end{align*}
 %]]&gt;</script>

<p>because</p>

<script type="math/tex; mode=display">
\sum_{i=1}^{N}{\left(x_i - \bar{x}_N\right)} = 0
</script>

<p>That’s enough for an algorithm, but not quite enough to get to the
pretty, easy-to-remember formula (don’t get me started on the
practical utility of beauty in mathematics – for another time!).</p>

<p>We just need to show that</p>

<script type="math/tex; mode=display">
N\gamma^2 + \left(x_{N+1}-\bar{x}_{N+1}\right)^2 = 
\left(x_{N+1}-\bar{x}_N\right)\left(x_{N+1}-\bar{x}_{N+1}\right)
</script>

<p>But</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}

x_{N+1}
  &=(N+1)\,\bar{x}_{N+1}-N\bar{x}_{N} \\
  &=N\gamma +\bar{x}_{N+1}
\end{align*}
 %]]&gt;</script>

<p>so</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}

\left( x_{N+1}-\bar{x}_{N+1}\right)^{2}
  &=N^{2}\gamma^{2}
  
\end{align*}
 %]]&gt;</script>

<p>Now, the term we need to analyze is</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}

N\gamma^{2}+\left(x_{N+1}-\bar{x}_{N+1}\right)^{ 2 }
  &=(N+N^2)\,\gamma^2\\
  &=N\gamma\,\left(N+1\right)\,\gamma
  
\end{align*}
 %]]&gt;</script>

<p>We already found that</p>

<script type="math/tex; mode=display">
N\gamma
  = x_{N+1}-\bar{x}_{N+1}
</script>

<p>so we just look at</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
\left(N+1\right)\,\gamma 
  &= \left(N+1\right)\left(\bar{x}_{N+1} - \bar{x}_N \right)\\
  &= \left(\left(N+1\right)\,\bar{x}_{N+1} - \left(N+1\right)\,\bar{x}_N \right)\\
  &= \left(\Sigma_{N+1} - \Sigma_N - \bar{x}_N \right)\\
  &= \left(x_{N+1} - \bar{x}_N \right)
\end{align*}
 %]]&gt;</script>

<p>Isn’t that fantastic?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fast Arbitrary Non-Uniform Randoms]]></title>
    <link href="http://rebcabin.github.com/blog/2012/12/13/fast-arbitrary-non-uniform-randoms/"/>
    <updated>2012-12-13T08:28:00-08:00</updated>
    <id>http://rebcabin.github.com/blog/2012/12/13/fast-arbitrary-non-uniform-randoms</id>
    <content type="html"><![CDATA[<p>You give me an array of integers representing the distribution of outcomes of some discrete random process (translation: you give me the specification of a loaded die: “I want 1 to come up 7/39 of the time; 2 to come up 5/39 of the time; I’d better NEVER see a 3 ‘cause I’m betting the farm on that; 4 to come up 11/39 of the time; 5 to come up 3/39 of the time; and 6 to come up 13/39 of the time.”).  You also give me a uniform pseudo-random number generator (PRNG) like Mathematica’s <a href="http://reference.wolfram.com/mathematica/ref/RandomInteger.html?q=RandomInteger&amp;lang=en">Random Integer</a>.  I give you back a new PRNG that honors the distribution you gave me, in the sense that, statistically, it generates outcomes with the same distribution as you gave me.</p>

<p>The problem is a practical, real-world problem.  Any time you need to generate random numbers according to some arbitrary, given distribution such as in simulation, traversing a Bayesian network, a decision tree, a packet retry backoff schedule, you name it, the problem comes up.  In my work, it comes up every time I need to generate random expressions for testing parsers or for Monte-Carlo search for formulas.  Most of the time, you want to generate expressions non-uniformly.  For instance, you’d like to generate addition expressions more frequently than division expressions or function calls more frequently than function definitions.  </p>

<p>There are a lot of solutions to this problem.  Probability mavens will immediately see that the problem boils down to <a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling">inverting the CDF</a>.  We can do this by:</p>

<ul>
  <li>linear search – O(N) space, O(N) time, where N is the number of different outcomes – the number of elements in the original array – the number of sides on the loaded die – 6 in our example</li>
  <li>binary search – O(N) space, O(log(N)) time – this works because the CDF is monotonically increasing</li>
  <li>by constructing an explicit inverse in an array – O(S) space, O(1) time, where S is the sum of the numbers in the array – the total number of trials imputed in the original distribution – 39 in our example </li>
  <li>by my favorite method: <a href="http://code.activestate.com/recipes/576564-walkers-alias-method-for-random-objects-with-diffe/">Walker’s method of aliases</a> – O(N) space and O(1) time.</li>
</ul>

<p>At first glance, this seems ridiculous, but there is a way.  In a nutshell: paint a dartboard with colors in the given proportions, throw darts, lookup the colors (thanks to Terence Spies for this beautiful idea).</p>

<p>Less metaphorically: imagine that the counts are colored balls distributed in bins:</p>

<ol>
  <li>7 balls of color number 1 in bin number 1</li>
  <li>5 balls of color number 2 in bin number 2</li>
  <li>0 balls of color number 3 in bin number 3</li>
  <li>11 balls of color number 4 in bin number 4</li>
  <li>3 balls of color number 5 in bin number 5</li>
  <li>13 balls of color number six in bin number 6</li>
</ol>

<p>Scale up the given counts of balls until the total is divisible by N – multiply every count by D/S where D=lcm(N,S)=lcm(6,39)=78: D/S=2</p>

<p>Now we have</p>

<ol>
  <li>14 balls of color number 1 in bin number 1</li>
  <li>10 balls of color number 2 in bin number 2</li>
  <li>0 balls of color number 3 in bin number 3</li>
  <li>22 balls of color number 4 in bin number 4</li>
  <li>6 balls of color number 5 in bin number 5</li>
  <li>26 balls of color number six in bin number 6</li>
</ol>

<p>Redistribute the counts amongst the bins so that every bin contains the same number of counts, namely D/N=13, and that the following conditions obtain:</p>

<ul>
  <li>every new bin contains at most 2 colors</li>
  <li>you never take ALL the balls out of any bin – every bin contains at least one ball of its original color if it had any</li>
</ul>

<p>There is a very elegant algorithm for this redistribution process.  It’s a one-liner whose statement is a proof of its correctness.  Think a minute, if you like, to see if you can come up with it, but here it is:</p>

<p>Fill up the shortest bin from the tallest bin; remove the new filled bin from the game and continue.</p>

<p>Do this procedure on a piece of paper:</p>

<ol>
  <li>Take 13 balls from bin 6 leaving 13 balls; fill up and remove bin 3</li>
  <li>Take 7 balls from bin 4 leaving 15 balls; fill up and remove bin 5</li>
  <li>Take 3 balls from bin 4 leaving 12 balls; fill up and remove bin 2</li>
  <li>Take 1 ball from bin 1 leaving 13 balls; fill up and remove bin 4</li>
  <li>Bins 1 and 6 are left with 13 balls each; we’re done (mechanically, you can imagine taking 0 balls from bin 6, filling up bin 1; removing bin 1, leaving just one bin – bin 6).</li>
</ol>

<p>you should end up with </p>

<ol>
  <li>13 balls of color 1 in bin 1 and no balls of any foreign color</li>
  <li>10 balls of color 2 and 3 balls of color 4 in bin 2</li>
  <li>0 balls of color 3 and 13 balls of color 6 in bin 3</li>
  <li>12 balls of color 4 and 1 balls of color 1 in bin 4</li>
  <li>6 balls of color 5 and 7 balls of color 4 in bin 5</li>
  <li>13 balls of color 6 in bin 6 and no balls of any foreign color</li>
</ol>

<p>Now, to generate new randoms, roll once to choose a bin <code>RandomInteger[{1,N}]</code>, and roll again to choose a height <code>RandomInteger[{1,D/N}]</code>.  If the randomly chosen height is less than or equal to the number of balls of the home color, return the number of the home color; otherwise, return the number of the foreign color. </p>

<p>Nifty, eh?  Here is a Mathematica notebook with an implementation tuned for clarity: <a href="https://dl.dropbox.com/u/1997638/FastNonUniformPseudoRandoms.cdf">https://dl.dropbox.com/u/1997638/FastNonUniformPseudoRandoms.cdf</a></p>
]]></content>
  </entry>
  
</feed>
